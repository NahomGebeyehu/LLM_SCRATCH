{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# LLM Data Pipeline: Tokenization & Sliding Windows\n",
    "\n",
    "**Purpose:** This notebook demonstrates how to tokenize a raw text file with `tiktoken`, build a sliding-window `Dataset` for autoregressive training, and create small batches for inspection. All code cells remain unchanged — only markdown and explanations were improved for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "* **Tokenization** — use `tiktoken` (GPT-2 BPE) to convert text to token IDs.\n",
    "* **Sliding window dataset** — produce overlapping input/target chunks for autoregressive training.\n",
    "* **Batching & embeddings** — create `DataLoader` batches and map tokens to dense vectors.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* `Data.txt` (a UTF-8 text file) in the same directory as the notebook.\n",
    "* `tiktoken` Python package (install with `pip install tiktoken`).\n",
    "* `PyTorch` available for `Dataset`, `DataLoader`, and tensors.\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1.  Setup and Installation\n",
    "2.  Data Loading & Tokenization\n",
    "3.  Quick tokenization check\n",
    "4.  Causal Modeling — input/target shift\n",
    "5.  Growing-context illustration\n",
    "6.  Sliding-window Dataset: idea & implementation\n",
    "7.  DataLoader factory\n",
    "8.  Sanity checks (reload & sample batches)\n",
    "9.  Token embeddings and shapes\n",
    "10. Create a small embedded batch (example)\n",
    "11. Embed inputs into 256-dimension vectors\n",
    "12. Notes & next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "Install the required tokenizer package if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Tokenization\n",
    "Read the source text and initialize the tokenizer. The following code reads `Data.txt` into `raw_text` and prepares `tiktoken` (GPT-2 encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "with open(\"Data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Quick tokenization check\n",
    "This short block prints the first character of the file, tokenizes with the **GPT-2 BPE encoder** and shows the token count and a short token sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenize-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(raw_text[:1])\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "enc_sample = enc_text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Causal Modeling — input/target shift (x, y example)\n",
    "**Autoregressive models** predict the next token given a context. Below we create `x` as a context of `context_size` tokens and `y` as the next token sequence (shifted by one, which is the model's target).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xy-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-md",
   "metadata": {},
   "source": [
    "## 5. Growing-context illustration (what the model sees)\n",
    "This loop demonstrates how the context grows token-by-token and the desired next token at each step. This visualizes the fundamental prediction task of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-context",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\"in interation {i}, with : {tokenizer.decode(context)} ==> : {tokenizer.decode([desired])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Sliding-window Dataset: idea & implementation\n",
    "**Idea:** From a single long token sequence, we produce many overlapping input/target examples.\n",
    "* For a `max_length` window, we take tokens `[i:i+max_length]` as **input**.\n",
    "* We take tokens `[i+1:i+max_length+1]` as the **target**.\n",
    "* The **stride** controls the amount of overlap between consecutive samples.\n",
    "\n",
    "**Why:** This approach maximizes training data utilization and ensures each token appears in several contexts. \n",
    "\n",
    "**Implementation (unchanged)** — this class produces `(input_ids, target_ids)` samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDataSetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk)) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. DataLoader factory\n",
    "The helper function below constructs the `GPTDataSetV1` and wraps it in a PyTorch `DataLoader`. This allows for efficient batching and multi-processing (controlled by `num_workers`). This cell remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataloader-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=6):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataSetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "## 8. Sanity checks (reload & sample batches)\n",
    "We reload the text (optional but kept for flow) and sample a couple of batches with a small `stride=1` and `max_length=4`. This clearly validates that the windowing and batching behave as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reload-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sanity-check",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "print(\"First Iter : \", first_batch)\n",
    "print(\"Second Iter : \", second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "## 9. Token embedding layer (shapes)\n",
    "We create a **token embedding layer** (`nn.Embedding`) to map the integer token IDs to dense, continuous vectors. This is the first step of the Transformer input process. Positional embeddings are typically added after this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedding-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# Shape note: If inputs has shape (B, T) then token_embedding_layer(inputs) => (B, T, output_dim)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "## 10. Create a small embedded batch (example)\n",
    "We instantiate a dataloader with a `batch_size=8` and `max_length=4`. The input tensor will have shape $(8, 4)$, which then gets embedded to $(8, 4, 256)$ when passed through `token_embedding_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-batch-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length =4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)\n",
    "#for i in range(5):\n",
    " #   print(\"\\nInput tokens : \\n\", tokenizer.decode(inputs[i].tolist()))\n",
    "    #print(\"\\n Inputs Shape: \\n\", inputs.shape)\\\n",
    "    \n",
    "  #  print(\"\\n Targets tokens : \\n\", tokenizer.decode(targets[i].tolist()))\n",
    "    #print(\"\\n Targets Shape: \\n\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11",
   "metadata": {},
   "source": [
    "## 11. Embed inputs into 256-dimension vectors\n",
    "The step to convert the token IDs to their dense vector representation, ready for the Transformer blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embed-call",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-12",
   "metadata": {},
   "source": [
    "## 12. Create another embedding layer for the postional encoder\n",
    "\n",
    "Add positional embeddings and implement the self-attention mechanism + transformer blocks to build a full model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c445d-a7c4-40d7-b4c0-e44ee752cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7da8b-a9b1-43ee-a7cf-85ba8eeb5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba6ab17",
   "metadata": {},
   "source": [
    "## 12. Implementation of a placeholder gpt model\n",
    "\n",
    "Just a dummy class with placeholders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef926f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ee7373",
   "metadata": {},
   "source": [
    "Behold the dummy class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e410a5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # Use a placeholder for TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # Use a placeholder for LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This block does nothing and just returns its input.\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # The parameters here are just to mimic the LayerNorm interface.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This layer does nothing and just returns its input.\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582c45b",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 2: **The Layer Normalization Part**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6fbf17",
   "metadata": {},
   "source": [
    "The layer normalization in the GPT architecture is crucial for two primary reasons:\n",
    "\n",
    "1.  **To mitigate vanishing or exploding gradients**, making the training process more stable.\n",
    "2.  **To reduce Internal Covariate Shift**, which refers to the problem of input distributions changing within the neural network layers during training.\n",
    "\n",
    "***\n",
    "\n",
    "The normalized values are adjusted to have a mean of zero and a variance of one.\n",
    "\n",
    "$$\n",
    "\\text{formula} = \\frac{(\\text{layer} - \\text{mean\\_of\\_layer})}{\\sqrt{\\text{variance\\_of\\_layer}}}\n",
    "$$\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee0eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example\n",
    "import torch, torch.nn as nn\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.rand(2,5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU()) # Relu for positive values onlly\n",
    "out = layer(batch_example)\n",
    "print(\"Out : \\n\", out)\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean : \\n \", mean)\n",
    "print(\"Variance : \\n \", var)\n",
    "out_norm = (out - mean)/ torch.sqrt(var)\n",
    "print(\"Normalized layer outputs : \\n\", out_norm)\n",
    "mean_norm = out_norm.mean(dim=-1, keepdim=True)\n",
    "var_norm = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Normalized Layers Mean: \\n\", mean_norm)\n",
    "print(\"Normalized Laters Variance \\n\", var_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303922e6",
   "metadata": {},
   "source": [
    "Layer normalizatoin as a class. The scale and shift are two trainable parameters (of the same dimension\n",
    "as the input) that the LLM automatically adjusts during training if it is determined that\n",
    "doing so would improve the model's performance on its training task. \n",
    "\n",
    "This allows the model\n",
    "to learn appropriate scaling and shifting that best suit the data it is processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c2583",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896c3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8148a594",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "\n",
    "As we can see based on the results, the layer normalization code works as expected and\n",
    "normalizes the values of each of the two inputs such that they have a mean of 0 and a\n",
    "variance of 1:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2126f1f",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 3: FEEDFORWARD NEURAL NETWORK WITH GELU ACTIVATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e5a30",
   "metadata": {},
   "source": [
    "The ReLu activation function is used to introduce non-linearility to the linear layers of the network, and to reduce the issue of the vanishing gradient problems. The ReLu sets all negative numbers to zero, which results in a zero gradient. If a neuron keeps on receiving negative inputs it can become permanently inactive or stop learning resulting in a Dead Neuron. The GeLu can allow small negative values to pass through it , and its also differentiable at every point, since it has a curvature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c84b0ca",
   "metadata": {},
   "source": [
    "Calculating the Gaussian CDF is computationally expensive, so there is a close approximation :\n",
    "\n",
    "$$GELU(x) \\approx 0.5x \\left( 1 + \\tanh \\left( \\sqrt{\\frac{2}{\\pi}} (x + 0.044715x^3) \\right) \\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42496c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):   # GeLu is the CDF of the standard guassian distribution\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2438d25",
   "metadata": {},
   "source": [
    "PLOT GELU VS RELU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3757c89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e5c845",
   "metadata": {},
   "source": [
    "Using the GELU class in a small FeedForward network, which will be used in the transformer block later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95fe047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1750f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c18819",
   "metadata": {},
   "source": [
    "The Feedforward Module, helps the model be able to learn and generalize the data. The input and output dimensions of this network is the same, it expands the embedding dimension into a higher dimensional space for a rich representation exploration. Also the uniformity of the input and output dimensions enables the stacking of layers which makes the model scalable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768) #A\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321bd3c",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 4: **Shortcut Connections**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd76b949",
   "metadata": {},
   "source": [
    "Vanishing gradients are a problem so that weights dont get updated which halts the learning process. Shortcut connections create an alternate path for the gradient to flow, skipping one or more layers. This is achieved by adding the output of one layer to the output of a latter layer. Also reduces the chances of local minimums occuring in the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c154d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1], GELU)),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2], GELU)),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3], GELU)),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4], GELU)),\n",
    "                nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5], GELU)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_output = layer(x)\n",
    "            # ts the place where shorti cuti\n",
    "            if self.use_shortcut and x.shape == layer_output.shape:\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "            \n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42065284",
   "metadata": {},
   "source": [
    "For now initialize the neural network without shortcut connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd664f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "torch.manual_seed(123)\n",
    "with_out = ExampleDeepNeuralNetwork(layer_sizes, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49076823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_gradient(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight'  in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df665444",
   "metadata": {},
   "source": [
    "In code above, there is a loss function that computes how close the output of the model is close to the one specified by the user. loss.backward() , this computes the loss gradient for each layer in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1b5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gradient(with_out, sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63623bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with = ExampleDeepNeuralNetwork(layer_sizes, True)\n",
    "print_gradient(model_with, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe544e7",
   "metadata": {},
   "source": [
    "When using a shortcut connection, it can be seen that instead of the weights vanishing as it approaches the layer 0, it stabilizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfda14c7",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 5: **The Attention and Linear Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4851107",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe24d6",
   "metadata": {},
   "source": [
    "Layer Normalization, GeLu and Feed-Forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f3ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de22d7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    " \n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x ): # GeLu is the CDF of the standard guassian distribution\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4be8bd",
   "metadata": {},
   "source": [
    "The Multi-Head attention Class from the Attention Mechanisms notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d48918",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads \n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_out, d_out)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) \n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3) \n",
    "\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        \n",
    "        return self.out_proj(context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf62448",
   "metadata": {},
   "source": [
    "Lets be coding on the **Transformer Block**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afeb540",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"image_26d267.png\" width=\"400px\">\n",
    "    <p>\n",
    "        <b>Figure 1: Transformer Block Architecture</b><br>\n",
    "        A single transformer layer showing the <b>Masked Multi-Head Attention</b> and \n",
    "        <b>Feed-Forward</b> sub-layers. Each sub-layer is wrapped in a \n",
    "        <b>Residual Connection</b> followed by <b>Layer Normalization</b>. \n",
    "        The Feed-Forward block uses a <b>GELU</b> activation between two linear layers \n",
    "        to process the (2, 4, 768) tensor.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2cbad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,cfg):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(\n",
    "            d_in = cfg[\"emb_dim\"],\n",
    "            d_out = cfg[\"emb_dim\"],\n",
    "            context_length = cfg[\"context_length\"],\n",
    "            num_heads = cfg[\"n_heads\"],\n",
    "            dropout = cfg[\"drop_rate\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x has a shape of [ batch_size, num_tokens, emb_size]\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attn(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8485f8",
   "metadata": {},
   "source": [
    "Instantiateeeee the transformer finally....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bdf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4 , 768)\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape: \", x.shape)\n",
    "print(\"Output shape : \", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6923204",
   "metadata": {},
   "source": [
    "this be showing that the dimension of the inputs and outputs are identical, this is important for shortcut(residual) connections, and also the output is rich with context and representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af37b3",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 6: **The enture gpt model architecture implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59be1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.1,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee19f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential( *[TransformerBlock(cfg) for _ in range (cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "\n",
    "        x = tok_embeds+ pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x) # the logits of the possible vocabs\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b029519e",
   "metadata": {},
   "source": [
    "Init a the 124 milli parameter model using the gpt config and pass an input with 2 batches and 4 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8227063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "batch = torch.tensor([[6109, 3626, 6100, 345], [6109, 1110, 6222, 257]])\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "output = model(batch)\n",
    "print(\"Input : \\n\", batch)\n",
    "print(\"Output shape : \", output.shape)\n",
    "print(\"Output : \\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc4dde",
   "metadata": {},
   "source": [
    "As seen the output has 2 batches , with 4 tokens with the vocab size of 50257 as the possible logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc6a246",
   "metadata": {},
   "source": [
    "**Using the numel() method, short for \"number of elements,\" we can collect the total\n",
    "number of parameters in the model's parameter tensors:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424ab55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f36b57",
   "metadata": {},
   "source": [
    "This number of paramters is higher than the expected 124M paramters, this is because in the GPT2 architecture, the paramters used for the token embedding layer where the same ones used in the linear output layer that is why it is less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef77c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97992a73",
   "metadata": {},
   "source": [
    "Both these weights have the sane shape, so if we remove this paramter count from the total and get 124 million"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ae8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ec30f",
   "metadata": {},
   "source": [
    "This reduces memory usage and computation cost of the model. It is said that using separate weights leads to better results and model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb605dd",
   "metadata": {},
   "source": [
    "the memory requirements of the 163 million parameters in GPTModel object: ( assuming that using float32 weights which is 4 bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e01e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7240e657",
   "metadata": {},
   "source": [
    "## GPT ARCHITECTURE PART 7: **Getting the text from the output of the GPT model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d850cd7",
   "metadata": {},
   "source": [
    "So basically the input(idx) is of shape batch, number of tokens which is the current context. Crop the current context(input), exceeds the supported context size. There is also a specified amount of maximum amount of newly generated tokens by the model. We take the output of the last token array, apply a softmax function to get it as a tensor of probailities and pick the index of the largest one and that will be the token id of the predicted word. Here the loss function wasnt measured, and the model isn't trained, just to demonstrate how the text is generated after the output of the GPT class/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0989b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text_v1(model, idx, max_new_tokens, context_size):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        with torch.no_grad(): # ts disables the autrograd engine , used for inference here so weights dont accidently be changing\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        logits = logits[:, -1, :] # ts takes the last time step/row from each batch [ batch, n_tokens, vocab_size]\n",
    "        probab = torch.softmax(logits, dim=-1)\n",
    "        idx_next = torch.argmax(probab, dim=-1, keepdim=True) # argmax returns the index of the one with the largest value\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # cat concatenates tensors across a specified dimension\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff3ca86",
   "metadata": {},
   "source": [
    "For now softax is kind of redundant, could literally just put logits in argmax but later on it is useful to use sampling techniques suxh that the model doesnt pick most likely token which brings variability and creativity in the output text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ead422",
   "metadata": {},
   "source": [
    "Generating text in the untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b74d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "init_context = \"The window was \"\n",
    "encoded = tokenizer.encode(init_context)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0) # unsqueeze to add dim to be a batch\n",
    "print(\"Encoded Tensor : \\n\", encoded_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea684aa",
   "metadata": {},
   "source": [
    "Since this is just inference and not training do model.eval() which bypasses dropout and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d7506",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out = gen_text_v1(\n",
    "    model=model,\n",
    "    idx=encoded_tensor,\n",
    "    max_new_tokens=6,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output : \", out)\n",
    "print(\"Decoded Output : \\n \", tokenizer.decode(out.squeeze(0).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2228a775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch CPU",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
