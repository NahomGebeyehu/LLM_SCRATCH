{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# LLM Data Pipeline: Tokenization & Sliding Windows\n",
    "\n",
    "**Purpose:** This notebook demonstrates how to tokenize a raw text file with `tiktoken`, build a sliding-window `Dataset` for autoregressive training, and create small batches for inspection. All code cells remain unchanged — only markdown and explanations were improved for clarity.\n",
    "\n",
    "---\n",
    "\n",
    "### Overview\n",
    "\n",
    "* **Tokenization** — use `tiktoken` (GPT-2 BPE) to convert text to token IDs.\n",
    "* **Sliding window dataset** — produce overlapping input/target chunks for autoregressive training.\n",
    "* **Batching & embeddings** — create `DataLoader` batches and map tokens to dense vectors.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* `Data.txt` (a UTF-8 text file) in the same directory as the notebook.\n",
    "* `tiktoken` Python package (install with `pip install tiktoken`).\n",
    "* `PyTorch` available for `Dataset`, `DataLoader`, and tensors.\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "1.  Setup and Installation\n",
    "2.  Data Loading & Tokenization\n",
    "3.  Quick tokenization check\n",
    "4.  Causal Modeling — input/target shift\n",
    "5.  Growing-context illustration\n",
    "6.  Sliding-window Dataset: idea & implementation\n",
    "7.  DataLoader factory\n",
    "8.  Sanity checks (reload & sample batches)\n",
    "9.  Token embeddings and shapes\n",
    "10. Create a small embedded batch (example)\n",
    "11. Embed inputs into 256-dimension vectors\n",
    "12. Notes & next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "Install the required tokenizer package if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Tokenization\n",
    "Read the source text and initialize the tokenizer. The following code reads `Data.txt` into `raw_text` and prepares `tiktoken` (GPT-2 encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "with open(\"Data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "## 3. Quick tokenization check\n",
    "This short block prints the first character of the file, tokenizes with the **GPT-2 BPE encoder** and shows the token count and a short token sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tokenize-check",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "5147\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:1])\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))\n",
    "enc_sample = enc_text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "## 4. Causal Modeling — input/target shift (x, y example)\n",
    "**Autoregressive models** predict the next token given a context. Below we create `x` as a context of `context_size` tokens and `y` as the next token sequence (shifted by one, which is the model's target).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "xy-shift",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [40, 367, 2885, 1464]\n",
      "y:     [367, 2885, 1464, 1807]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1]\n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y:     {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-5-md",
   "metadata": {},
   "source": [
    "## 5. Growing-context illustration (what the model sees)\n",
    "This loop demonstrates how the context grows token-by-token and the desired next token at each step. This visualizes the fundamental prediction task of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "growing-context",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in interation 1, with : I ==> :  H\n",
      "in interation 2, with : I H ==> : AD\n",
      "in interation 3, with : I HAD ==> :  always\n",
      "in interation 4, with : I HAD always ==> :  thought\n"
     ]
    }
   ],
   "source": [
    "for i in range (1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "    print(f\"in interation {i}, with : {tokenizer.decode(context)} ==> : {tokenizer.decode([desired])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-6",
   "metadata": {},
   "source": [
    "## 6. Sliding-window Dataset: idea & implementation\n",
    "**Idea:** From a single long token sequence, we produce many overlapping input/target examples.\n",
    "* For a `max_length` window, we take tokens `[i:i+max_length]` as **input**.\n",
    "* We take tokens `[i+1:i+max_length+1]` as the **target**.\n",
    "* The **stride** controls the amount of overlap between consecutive samples.\n",
    "\n",
    "**Why:** This approach maximizes training data utilization and ensures each token appears in several contexts. \n",
    "\n",
    "**Implementation (unchanged)** — this class produces `(input_ids, target_ids)` samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dataset-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class GPTDataSetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk)) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-7",
   "metadata": {},
   "source": [
    "## 7. DataLoader factory\n",
    "The helper function below constructs the `GPTDataSetV1` and wraps it in a PyTorch `DataLoader`. This allows for efficient batching and multi-processing (controlled by `num_workers`). This cell remains unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dataloader-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=6):\n",
    "    \n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDataSetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-8",
   "metadata": {},
   "source": [
    "## 8. Sanity checks (reload & sample batches)\n",
    "We reload the text (optional but kept for flow) and sample a couple of batches with a small `stride=1` and `max_length=4`. This clearly validates that the windowing and batching behave as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "reload-text",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sanity-check",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Iter :  [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n",
      "Second Iter :  [tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "second_batch = next(data_iter)\n",
    "print(\"First Iter : \", first_batch)\n",
    "print(\"Second Iter : \", second_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-9",
   "metadata": {},
   "source": [
    "## 9. Token embedding layer (shapes)\n",
    "We create a **token embedding layer** (`nn.Embedding`) to map the integer token IDs to dense, continuous vectors. This is the first step of the Transformer input process. Positional embeddings are typically added after this step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "embedding-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size= 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "\n",
    "# Shape note: If inputs has shape (B, T) then token_embedding_layer(inputs) => (B, T, output_dim)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-10",
   "metadata": {},
   "source": [
    "## 10. Create a small embedded batch (example)\n",
    "We instantiate a dataloader with a `batch_size=8` and `max_length=4`. The input tensor will have shape $(8, 4)$, which then gets embedded to $(8, 4, 256)$ when passed through `token_embedding_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "final-batch-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length =4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs,targets = next(data_iter)\n",
    "#for i in range(5):\n",
    " #   print(\"\\nInput tokens : \\n\", tokenizer.decode(inputs[i].tolist()))\n",
    "    #print(\"\\n Inputs Shape: \\n\", inputs.shape)\\\n",
    "    \n",
    "  #  print(\"\\n Targets tokens : \\n\", tokenizer.decode(targets[i].tolist()))\n",
    "    #print(\"\\n Targets Shape: \\n\", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-11",
   "metadata": {},
   "source": [
    "## 11. Embed inputs into 256-dimension vectors\n",
    "The step to convert the token IDs to their dense vector representation, ready for the Transformer blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "embed-call",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "print(token_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-12",
   "metadata": {},
   "source": [
    "## 12. Create another embedding layer for the postional encoder\n",
    "\n",
    "Add positional embeddings and implement the self-attention mechanism + transformer blocks to build a full model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c11c445d-a7c4-40d7-b4c0-e44ee752cde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_size, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2bf7da8b-a9b1-43ee-a7cf-85ba8eeb5ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "print(pos_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f05e1ec-3baf-4b54-ad32-54cb5149077c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torchtest)",
   "language": "python",
   "name": "torchtest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
