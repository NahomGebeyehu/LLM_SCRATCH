{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554eabc7-e7c5-484d-b55f-b7769d40f6be",
   "metadata": {},
   "source": [
    "#  Part 1: Creating a Simplified Attention Mechanism\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f91ad3d-f653-43f9-8c3a-5d1489d652a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], #Your\n",
    "     [0.55, 0.87, 0.66], #Journey\n",
    "     [0.57, 0.85, 0.64], #Starts\n",
    "     [0.22, 0.58, 0.33], #With\n",
    "     [0.77, 0.25, 0.10], #One\n",
    "     [0.05, 0.80, 0.55]] #Step\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b71f59-ec4c-42d1-aabd-a19c17eb5655",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">PLOT THE SIMILARITY BETWEEN WORDS USING WORD EMBEDDINGS</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Visualizing high-dimensional vector relationships in 3D space to observe how the model clusters semantically related tokens, providing a clear window into the model's internal understanding of language.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8e13e-aa53-44cc-9369-c56bb1d7cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "words = ['Your', 'journey', 'starts', 'with', 'one', 'step']\n",
    "x_coords = inputs[:, 0].numpy()\n",
    "y_coords = inputs[:, 1].numpy()\n",
    "z_coords = inputs[:, 2].numpy()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for x, y,z, word in zip(x_coords, y_coords, z_coords, words):\n",
    "    ax.scatter(x,y,z)\n",
    "    ax.text(x,y,z, word, fontsize=10)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_xlabel('Z')\n",
    "\n",
    "plt.title(\"A tale of 3D word embeddings\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4db9f7-440a-4b1d-9280-3f5f1c88af44",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">CODE A SIMPLE DOT PRODUCT BASED ATTENTION SCORE</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Implementing the fundamental mathematical operation where the dot product between query and key vectors measures their alignment, determining how much focus one token should place on another.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2f2c5-0eb3-4cb3-82b1-c17ef159a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = inputs[1]\n",
    "print(\"Inputs Shape :\", inputs.shape)\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "print(\"Init Attention Scores: \", attn_scores_2)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    print(\"The I : \", i , \"The XI : \", x_i)\n",
    "    attn_scores_2[i] = torch.dot(x_i , query)\n",
    "\n",
    "print(\"Attention Scores : \", attn_scores_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9e4e19-4810-47ba-8864-d0f6c5954dc5",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">NORMALIZE ATTENTION SCORE TO REPRESENT IT AS A PERCENTAGE OF WEIGHTS THAT SUM UP TO ONE</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Applying the Softmax transformation to convert raw similarity scores into a probability distribution, ensuring that the influence of all input tokens is balanced and relative to one another.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2b006-4335-4276-ae6e-eeebb2a3944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sum_scores = attn_scores_2.sum()\n",
    "print(\"total sum : \" , total_sum_scores)\n",
    "normalized_attention_scores = attn_scores_2 / total_sum_scores;\n",
    "print(\"Normalized attention scores : \", normalized_attention_scores)\n",
    "print(\"Sum of normalized attention scores : \", normalized_attention_scores.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56143b85",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">USING SOFTMAX NORMALIZATION FOR A DIFFERENTIABLE GRADIENT</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Leveraging the Softmax function not just for probability distribution, but to maintain a smooth, differentiable surface that allows backpropagation to effectively update weights across the entire network.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3670cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_scores_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Softmax normalized attention weights : \", attn_scores_2_naive)\n",
    "print(\"Sum: \", attn_scores_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594fb8b",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #fafafa;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #333;\">USING PYTORCH'S IMPLEMENTATION OF SOFTMAX:</p>\n",
    "    <p style=\"margin: 10px 0 0 0; font-family: monospace; font-size: 1.1em; color:black\">\n",
    "        exp(xi) / Σ exp(xj) \n",
    "    </p>\n",
    "    <p style=\"margin: 5px 0 0 0; font-size: 0.9em; color: #666;\">(SAFE FOR VERY LARGE OR SMALL VALUES)</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b67a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "print(\"Attention Weights : \", attn_weights_2)\n",
    "print(\"Sum: \", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4111bae",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #d1d1d1; border-radius: 4px; background-color: #fcfcfc;\">\n",
    "    <span style=\"text-transform: uppercase; font-weight: bold; color: #444; letter-spacing: 0.5px;\">\n",
    "        The context vector is calculated as a weighted sum of all input vectors\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a44999",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = inputs[1]\n",
    "print(query.shape)\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "    print(\"ATW* X_I: \\n\", attn_weights_2[i] * x_i)\n",
    "    print(\"CV in loop : \\n\", context_vec_2)\n",
    "print(\"Context Vector : \\n\", context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e6e5b",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">CAN CREATE A TENSOR REPRESENTING AN ATTENTION SCORE BETWEEN EACH PAIR OF INPUTS</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Since for loops are generally slow, same results can be achieved using matrix multiplication)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c95cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inputs : \\n \", inputs)\n",
    "print(\"Inputs Tranposed : \\n \", inputs.T)\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(\"Attention Scores :\\n \", attn_scores)\n",
    "print(\"Attention Scores Shapes : \", attn_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ae1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_attention_scores = torch.softmax(attn_scores,dim=-1)\n",
    "print(\"Normalized attention scores : \\n \", normalized_attention_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbd3b53",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">COMPUTE ALL CONTEXT VECTORS</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Synthesizing the final context vectors by calculating the weighted sum of the value vectors. This step aggregates information from the entire sequence based on the normalized attention scores, producing the enriched representation for each token.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39128cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_context_vectors = normalized_attention_scores @ inputs\n",
    "print(\"All context vectors : \\n\", all_context_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7c1e4",
   "metadata": {},
   "source": [
    "#  Part 2: Creating a Self Attention Mechanism With Trianable Weights (SCALED)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ef53ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "inputs = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89], #Your\n",
    "     [0.55, 0.87, 0.66], #Journey\n",
    "     [0.57, 0.85, 0.64], #Starts\n",
    "     [0.22, 0.58, 0.33], #With\n",
    "     [0.77, 0.25, 0.10], #One\n",
    "     [0.05, 0.80, 0.55]] #Step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = inputs.shape[1]\n",
    "d_out = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bbebb3",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">INITIALIZE THE WEIGHT MATRICES OF QUERY, KEY AND VALUE</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50139f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "print(\"The Weights Query : \\n\", W_query)\n",
    "print(\"The Weights Key : \\n\", W_key)\n",
    "print(\"The Weights Value : \\n\", W_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5678c74a",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">COMPUTE QUERY, KEY AND VALUE VECTORS</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Applying the weight matrices to the input embeddings to generate the specific Q, K, and V representations)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_2 = inputs @ W_query\n",
    "key_2 = inputs @ W_key\n",
    "value_2 = inputs @ W_value\n",
    "print(\"Query : \\n\", query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ W_key\n",
    "query = inputs @ W_query\n",
    "value =  inputs @ W_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584c6a65",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">WE CAN ATTEND TO HOW THE QUERY MATRIX ATTENDS TO THE KEY MATRIX BY DOING A DOT PRODUCT OR TRANSPOSED MATRIX MULTIPLICATION</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Calculating the raw attention scores by measuring the alignment between queries and the transposed key matrix)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e82812b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_2 = keys[1]\n",
    "print(\"Shaopes :\", key_2.shape , query_2.shape)\n",
    "attn_scores_22 = query_2[1] @ keys.T\n",
    "print(\"Attention Scores : \",attn_scores_22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a60cd33",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">GET ALL ATTENTION SCORES VIA MATRIX MULTIPLICATIONS</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Performing a single optimized matrix operation to compute the pairwise compatibility scores for the entire sequence)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a5173",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores = query @ keys.T\n",
    "print(\"All attention scores : \\n\", attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db22db1",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">COMPUTE ATTENTION WEIGHTS BY SCALING THE ATTENTION SCORES AND USING SOFTMAX FUNCTION , WE SCALE THE ATTENTION SCORES BY DIVIDING THEM BY THE SQUARE ROOT OF THE EMBEDDING DIMENSIONS (I.E √2), MATHEMATICALLY THE SAME AS EXPONENTIATING BY 0.5 [TO AVOID VANISHING GRADIENTS, OVER CONFIDENCE, LEARNING UNSTABILITY AND REDUCE VARIANCE WHICH GROWS LINERALLY WITH THE DIMENSION OF THE MATIRCES]</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "         Without this scaling factor ($\\sqrt{d_out}$), the dot products grow large in magnitude, pushing the softmax function into regions where gradients are extremely small, effectively \"killing\" the ability of the model to learn during backpropagation.\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc9c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_k = keys.shape[-1]\n",
    "print(\"Embedding dimensions: \", d_k)\n",
    "attn_weights = torch.softmax(attn_scores / d_k**0.5 , dim=-1)\n",
    "print(\"Scaled Attentions with sqrt : \\n \", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db3ba0e",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">CALCULATING THE CONTEXT VECTOR BY BY MATRIX MULTIPLACTIN OF THE WEIGHTING FACTOR (ATTN_WEIGHTS) AND THE VALUE VECTOR</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (This operation produces the final output of the attention mechanism by linearly combining the values based on their calculated importance scores.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a0958",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec = attn_weights @ value\n",
    "print(\"Context vectors : \\n\", context_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c95e56",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">ORGANIZE THE SELF ATTENTION MECHANISM AS A PYTHON CLASS</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Encapsulating the logic into a reusable PyTorch module that maintains the learnable weight matrices and handles the forward pass efficiently.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58db2081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "\n",
    "        attn_scores = query @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66147ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(\"Context vectors : \\n \", sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a352364",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">IMPROVE THE SELF ATTENTION CLASS BY USING TORCH'S LINEAR LAYERS, IT DOES MATRIX MULTIPLICATION WHEN BIAS UNITS ARE DISABLED, NO NEED TO MANUALLY IMPLEMENT NN.PARAMETER, AND ALSO HAS AN OPTIMIZED WEIGHT</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec642b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = query @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7151a8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(\"Context vectors : \\n \", sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c3889",
   "metadata": {},
   "source": [
    "#  Part 3: Creating a Casual/Masked Self Attention Mechanism\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e5eec",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">FIRSTLY WE GET THE ATTENTION WEIGHTS FROM THE ATTENTION SCORES THEN , WE HIDE FUTURE OR SUBSEQUENT WORDS WORDS</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Implementing causal masking to ensure that the model can only attend to previous and current positions, preventing information leakage from future tokens during training.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "print(\"Attention Weights : \\n\", attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b67dd",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">CREATE A MASK WHICH AS A TRIANGLE LOWCUT MATRIX WITH SCALAR VALUES OF 1 , WHICH WE CAN MULTIPLY THE ATTENTION WEIGHTS TO</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Generating a lower-triangular matrix of ones that, when element-wise multiplied with the attention weights, zero-out the connections to future tokens.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0992e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = attn_scores.shape[0]\n",
    "simple_mask = torch.tril(torch.ones(context_length, context_length))\n",
    "print(\"The mask : \\n\", simple_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d973ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_weights = attn_weights * simple_mask\n",
    "print(\"Masked weights : \\n\", masked_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdc982f",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">NOW WE GOTTA NORMALIZE EACH ROW TO SUM UP TO ONE BECAUSE ITS AN ATTENTION WEIGHT SO A SIMPLE NORMALIZATION WILL DO</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Dividing each masked attention weight by the sum of its row to ensure the total probability distribution across all valid previous tokens equals 1.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb7be8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_of_rows = masked_weights.sum(dim=-1, keepdim=True)\n",
    "masked_weights = masked_weights / sum_of_rows\n",
    "hmmm = torch.tensor([0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000])\n",
    "print(\"Hmm sum : \", hmmm.sum())\n",
    "print(\"Masked Weights \\n :\", masked_weights )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42348ba3",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">MASKING AFTER SOFTMAX NORMALIZATION MAKES IT INFLUENCED BY SUBSEQUENT TOKENS, THUS A KIND OF DATA LEAK. SO IF WE APPLY AN UPPER INFINITY TRIANGULAR MASK, WHICH WHEN WHEN EXPONENTIATED GIVES ZERO , WHICH AFTER WILL BE NORMALIZED WITH SOFTMAX WHICH THE MAKSED VALUE DOESNT AFFECT THE OTHERS</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (By filling the upper triangle of the raw scores with $-\\infty$ before the softmax, the exponentiation $e^{-\\infty}$ results in 0, effectively removing future tokens from the probability distribution calculation entirely.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d6a2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "masked_scores = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"Masked Score s: \\n\", masked_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4950b",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_weights =  torch.softmax(masked_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(\"Attention Weights Masked : \\n\", masked_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dec864",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_vec = masked_weights @ sa_v2.W_value(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7a9a0",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">DROPOUT IN CAUSAL ATTENTION HELPS PREVENT OVERFITTING. SINCE ATTENTION HAS LOTS OF PARAMETERS, IT CAN LOCK ONTO TRAINING DATA TOO CLOSELY. BY DROPPING SOME ATTENTION WEIGHTS RANDOMLY DURING TRAINING, THE MODEL LEARNS MORE DIVERSE CONNECTIONS AND DEVELOPS ROBUST, GENERALIZABLE PATTERNS, WHICH IS IMPORTANT FOR COHERENT FUTURE TEXT. TO COMPHENSATE THE OTHERS ELEMENTS ARE SCALED BY A FACTOR OF 2 IN THIS CASE. ([1-1/P] KIND OF BASED ON BERNOULLI DISTRIBUTIONS)</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Applying a Bernoulli mask to the attention weights to ensure that no single token relationship dominates the learning process, maintaining expected values through inverse-probability scaling.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "dropout_mask = dropout(masked_weights)\n",
    "print(\"Dropout Mask : \\n\", dropout_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696da77",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">NOW CODING THE CAUSAL ATTENTION MECHANISM AS A CLASS , ALOS MAKING IT SO THAT IT CAN HANDLE BATCHES CONSISTING MORE THAT INPUTS, THIS ENSURES THAT THE CLASS SUPPORTS BATCH OUTPUTS FROM THE GPTDATASETV1 DATA LOADER AND DATASET CLASSES IMPLEMENTED IN THE OTHER NOTEBOOK</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e94ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(\"Simulated Batc Shape: \\n\", batch.shape)\n",
    "print(\"Batch : \\n\", batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab3310f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CasualAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout_rate, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length,context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        buff_index, num_tokens, d_in = x.shape\n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "        attn_scores =  attn_scores.masked_fill(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "ca = CasualAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vecs = ca(batch)\n",
    "print(\"Context Vectors : \\n\", context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67abe99",
   "metadata": {},
   "source": [
    "#  Part 4: Creating a Multi-Head Self Attention Mechanism\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd93fa8",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">THIS MECHANISMS CREATES MULTIPLE INSTANCES OF THE SELF-ATTENTION MECHANISM , WITH EACH WITH ITS OWN WEIGHT MATRICES AND OUTPUTS, THEN CONCATINATE THE CONTEXT VECTORS FROM THE ATTENTION HEADS INTO ONE CONTEXT VECTOR ACROSS THE COLUMNS</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Implementing Multi-Head Attention to allow the model to jointly attend to information from different representation subspaces at different positions.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttenitionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout_rate, num_heads, qkv_bias=False ):\n",
    "        super().__init__()\n",
    "        self.heads= nn.ModuleList([\n",
    "            CasualAttention(d_in,d_out, context_length, dropout_rate, qkv_bias)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f197db",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttenitionWrapper(d_in,d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(\"Context Vectors shape : \", context_vecs.shape)\n",
    "print(\"Context Vectors : \\n\", context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61be257e",
   "metadata": {},
   "source": [
    "#  Part 5: Implementing a Multi-Head Self Attention Mechanism With Split Weights\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ee574a",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid #e0e0e0; border-radius: 5px; background-color: #f9f9f9;\">\n",
    "    <p style=\"margin: 0; font-weight: bold; color: #222;\">INSTEAD OF HAVING A WRAPPER AND A CASUAL ATTENTION CLASS, WE CAN COMBINE INTO A SINGLE CLASS WITH MORE ATTENTION TO EFFICIECCY</p>\n",
    "    <hr style=\"margin: 10px 0; border: 0; border-top: 1px solid #ddd;\">\n",
    "    <p style=\"margin: 0; font-size: 0.95em; color: #555; font-style: italic;\">\n",
    "        (Rather than instantiating multiple independent heads, we use a single large weight matrix and split it into multiple subspaces using tensor reshaping. This \"split weights\" approach allows for highly optimized, parallelized matrix multiplications across all heads simultaneously.)\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bcf680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads \n",
    "\n",
    "        # Projections to transform input into Query, Key, and Value spaces\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        \n",
    "        # Final linear layer to merge concatenated heads\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Causal mask: upper triangular matrix to prevent attending to future tokens\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # 1. Linear projections -> (b, num_tokens, d_out)\n",
    "        keys = self.W_key(x) \n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # 2. Split d_out into heads -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 3. Transpose to move heads to the front -> (b, num_heads, num_tokens, head_dim)\n",
    "        # This allows us to perform matrix multiplication on each head independently.\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # 4. Compute Attention Scores (Dot Product)\n",
    "        # (b, nh, T, hd) @ (b, nh, hd, T) -> (b, nh, T, T)\n",
    "        # We transpose the last two dims of 'keys' so the head_dims align for multiplication.\n",
    "        attn_scores = queries @ keys.transpose(2, 3) \n",
    "\n",
    "        # 5. Apply Causal Mask\n",
    "        # We slice the pre-computed mask to the current sequence length (num_tokens)\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        # 6. Normalize Scores (Softmax + Scaling)\n",
    "        # Scaling by sqrt(head_dim) prevents gradients from vanishing\n",
    "        attn_weights = torch.softmax(attn_scores / self.head_dim**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # 7. Compute Context Vector\n",
    "        # (b, nh, T, T) @ (b, nh, T, hd) -> (b, nh, T, hd)\n",
    "        # Then move num_heads back to merge: -> (b, T, nh, hd)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # 8. Concatenate heads back into one vector -> (b, T, d_out)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        \n",
    "        # 9. Final output projection\n",
    "        return self.out_proj(context_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd6d784",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttention(d_in,d_out, context_length, 0.0, num_heads=2)\n",
    "context_vecs = mha(batch)\n",
    "print(\"Context Vectors shape : \", context_vecs.shape)\n",
    "print(\"Context Vectors : \\n\", context_vecs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch CPU",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
